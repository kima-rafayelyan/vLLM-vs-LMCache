# vLLM-vs-LMCache

ðŸš€ vLLM vs vLLM + LMCache Benchmark

This project benchmarks large language model inference performance using vLLM with and without LMCache, demonstrating how KV cache reuse and prefix caching can dramatically reduce latency and improve throughput for long-context workloads.

**Overview**

Modern LLM applications often reuse long shared prefixes (system prompts, documents, RAG contexts).
Recomputing these prefixes for every request is expensive.

This benchmark compares:

â€¢ Baseline: vLLM without prefix caching

â€¢ Optimized: vLLM + LMCache with KV cache reuse

The goal is to quantify:

â€¢ Latency improvement

â€¢ Throughput improvement

â€¢ GPU memory trade-offs
