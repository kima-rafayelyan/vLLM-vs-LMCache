{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Environment Setup and Library Imports**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dYvWMTdZB7vb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQesyTg7QSU-",
        "outputId": "e8e41206-cd5c-4235-ce07-c6eeae6fbfb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.9/87.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.6/192.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m798.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m120.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.0/457.0 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.5/323.5 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.0/73.0 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.0/388.0 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.4/285.4 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m112.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.0/149.0 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.9/394.9 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m111.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.6/821.6 kB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m959.8/959.8 kB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 3.0.1 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.33.5 which is incompatible.\n",
            "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.5 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.5 which is incompatible.\n",
            "dask-cudf-cu12 25.10.0 requires pandas<2.4.0dev0,>=2.0, but you have pandas 3.0.1 which is incompatible.\n",
            "gradio 5.50.0 requires pandas<3.0,>=1.0, but you have pandas 3.0.1 which is incompatible.\n",
            "bqplot 0.12.45 requires pandas<3.0.0,>=1.0.0, but you have pandas 3.0.1 which is incompatible.\n",
            "db-dtypes 1.5.0 requires pandas<3.0.0,>=1.5.3, but you have pandas 3.0.1 which is incompatible.\n",
            "cudf-cu12 25.10.0 requires pandas<2.4.0dev0,>=2.0, but you have pandas 3.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -U -q vllm lmcache transformers accelerate pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import gc\n",
        "import subprocess\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from vllm import LLM, SamplingParams\n",
        "from vllm.config import KVTransferConfig"
      ],
      "metadata": {
        "id": "oP06EULc-jVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block specifies the Hugging Face model repository to be used for inference and the *get_gpu_memory* function checks the current GPU memory usage (in MB)\n"
      ],
      "metadata": {
        "id": "XIVb0XNsCrQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "\n",
        "\n",
        "def get_gpu_memory():\n",
        "   result = subprocess.check_output(\n",
        "       [\"nvidia-smi\", \"--query-gpu=memory.used\", \"--format=csv,noheader,nounits\"]\n",
        "   )\n",
        "   return int(result.decode(\"utf-8\").strip())\n"
      ],
      "metadata": {
        "id": "FO_e06oc-n0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a document about machine learning fundamentals and repeat it 400 times. This creates a massive context window."
      ],
      "metadata": {
        "id": "SVOKXWKPXYN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shared_prefix = \"\"\"\n",
        "You are an AI assistant that answers questions based on the following document.\n",
        "\"\"\" + (\n",
        "   \"\"\"\n",
        "Machine learning is a field of artificial intelligence that focuses on building systems that learn from data.\n",
        "Supervised learning uses labeled datasets to train models.\n",
        "Unsupervised learning finds patterns without labels.\n",
        "Reinforcement learning is based on reward-driven agents interacting with environments.\n",
        "Neural networks are inspired by biological neurons and consist of layers of interconnected nodes.\n",
        "Deep learning uses many layers to model complex patterns.\n",
        "\"\"\" * 400   # long context to expose lmcache benefit\n",
        ")\n",
        "\n",
        "\n",
        "questions = [\n",
        "   \"What is supervised learning?\",\n",
        "   \"What is unsupervised learning?\",\n",
        "   \"What is reinforcement learning?\",\n",
        "   \"What are neural networks?\",\n",
        "   \"What is deep learning?\"\n",
        "]\n",
        "\n",
        "\n",
        "NUM_REQUESTS = 20\n",
        "\n",
        "\n",
        "prompts = [\n",
        "   shared_prefix + \"\\nQuestion: \" + questions[i % len(questions)]\n",
        "   for i in range(NUM_REQUESTS)\n",
        "]\n",
        "\n",
        "\n",
        "sampling_params = SamplingParams(\n",
        "   temperature=0.0,\n",
        "   max_tokens=50\n",
        ")\n"
      ],
      "metadata": {
        "id": "jQdWTAvX-toq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a benchmarking function that measures how quickly the model processes the batch of requests."
      ],
      "metadata": {
        "id": "GhRJTtSndJI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_benchmark(llm, prompts, label):\n",
        "\n",
        "\n",
        "   torch.cuda.synchronize()\n",
        "\n",
        "\n",
        "   start_time = time.time()\n",
        "   vllm_results = llm.generate(prompts, sampling_params)\n",
        "   torch.cuda.synchronize()\n",
        "   end_time = time.time()\n",
        "\n",
        "\n",
        "   total_time = end_time - start_time\n",
        "   latency = total_time / len(prompts)\n",
        "   throughput = len(prompts) / total_time\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   print(f\"\\n--- {label} ---\")\n",
        "   print(f\"Total time: {total_time:.3f} s\")\n",
        "   print(f\"Latency: {latency:.2f} s\")\n",
        "   print(f\"Throughput: {throughput:.2f} req/s\")\n",
        "\n",
        "\n",
        "   return {\n",
        "       \"latency\": round(latency,2),\n",
        "       \"throughput\": round(throughput,2),\n",
        "   }\n"
      ],
      "metadata": {
        "id": "vF1X4PdX-0mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block runs the model using standard vLLM settings without any external caching mechanisms. This represents a \"Cold Run\" where the system must re-process the long context for every request in the batch."
      ],
      "metadata": {
        "id": "shYTXLVWdVgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n===== BASELINE (vLLM only) =====\")\n",
        "mem_before_vllm = get_gpu_memory()\n",
        "llm_baseline = LLM(\n",
        "    model=MODEL_NAME,\n",
        "    gpu_memory_utilization=0.7,\n",
        "    enable_prefix_caching=False\n",
        ")\n",
        "mem_after_vllm = get_gpu_memory()\n",
        "vllm_mem_used = mem_after_vllm - mem_before_vllm\n",
        "print(f\"vLLM reserved: {vllm_mem_used} MB\")\n",
        "\n",
        "baseline_results = run_benchmark(llm_baseline, prompts, \"vLLM Cold Run\")\n",
        "\n",
        "# Cleanup\n",
        "del llm_baseline\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "IflFQkGk-1-o",
        "outputId": "6a78ccef-fec1-4205-f680-67582daf4cd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692,
          "referenced_widgets": [
            "d635326e6f44458e891dbebee355cd26",
            "79feee6a685c472dbdc42510389e6a2b",
            "b116837d1a054f71a69e3c51f51707de",
            "8f1bf29246c947ebaf22315fa1a57e74",
            "c2ee3190f79a45d99832592be9a1de34",
            "e09be083981a4393a707d1d3867a37b2",
            "06787e61f19c437ca80dd6b2517382d0",
            "9866a5c26a034bd699b3a80f8d58e6b0",
            "bbca61b1ad374fbf916bb3a34522354f",
            "481201e9e8b446efbe6845910fb877e8",
            "7be459f6a78b481fbe97c31b77cc4b85",
            "fede554e895743769362628ccb59e31d",
            "519f664649774656be855532cd966226",
            "e8fc5d55aa024b5594830d9286f9868f",
            "d1d331052f624d45a5f0db5ff09077f8",
            "22b7c13258d642f8ac89ce3bdea2bf63",
            "989a8f84da7442e2a499d8f79ccebbd1",
            "c907a23819164fd1a525fbac8e62944f",
            "dd5c1926ff2b442797b666d2d3952207",
            "91df44c22456414a9485a5a30182542a",
            "5ad96217b5f245178c28dc3f1c02371c",
            "c6b573aa219048f7aae7879fac02f229",
            "648b44dd505943ecb1945d846b949240",
            "b499cdebc41048b29ee966caa6e4b09e",
            "ec4fe2f5e2e7431987166172fafdeb08",
            "4f474840f05f42d7ae3046e1db42f2fe",
            "c1c05c7794444a938d71becbe93e6b9d",
            "d6ebcde61ac8419fa5f6019de67f0603",
            "393c937c399647619ed4a70c9206f89d",
            "5e8da12cca6244a8a0890d18e8247beb",
            "30e160367e7e42a1a6e155564d283368",
            "b7f046e2a56e4843aba591fc0363a870",
            "fb290d6324974f67ac0fe4d67795c90b",
            "ff314c0dc96d4742a9f244c08975c10d",
            "7c59516dca6b4b72983bfedfa26449ba",
            "70769169698c4e93919e2b5e6c3a9966",
            "16c43eecec6b4c4789473af581ce4de4",
            "79fe25b743344b948f7d7d5a56308e9d",
            "f09073a32afa40e88fec0ec08db96ca4",
            "053a168b5ab44f3e92886adcd9f8e13b",
            "b46350d8c2c24a2aa762c5c7e1d86726",
            "9690daf75a8f46b1989936387eb31fad",
            "912dda5a1db64a3482cb1a5c1354144f",
            "67b8fbe7701d462f98870920a217eff6",
            "bd927cdc8bfc4c6c92482b1507fe7dc7",
            "9418f3218df0449985cfe4f92c431d0d",
            "0fb69e9079654185b2abfc16509c5916",
            "880ac5b7ddea478ca88064e69b6476fd",
            "b5aa8c551c2a40a28132b58b396c4293",
            "0367a291df2d46fdb1bf6f0c0a22a0cb",
            "375834ece7f74a02abc083c71c600f15",
            "bdcd46675e2843bbae6d2e0564ad3fff",
            "b14b4843ccd045db8ad93e0a6f7db7f1",
            "5d62314296e243879323cc26198fdfc3",
            "e5cbf421834d43fa8c57bfd63234e24c",
            "438950fd7cf8411abd543ba98786c0ac",
            "5a4f894e5ff04577b18ee19b78bc40ab",
            "a1e5055626f24903ac80340181eeb49f",
            "d90acfd5068542f8aca336c5fb3e72bf",
            "634b68fa626d429f95eaa95bd08f1c64",
            "4a23e9444004464ba3bc3da6177136b6",
            "32d531dd12284ce9b773257a84964e56",
            "d7b2370d411d4b919815d163d061a868",
            "ae934281c4f344c2b199bd1c4aa9303c",
            "ad2f22516cb9411681d1d44fd201100c",
            "99bbe709ee92408584a3c838f2539847",
            "9b93efeeebeb47fc804bce87af5b79a7",
            "2b3171d71648457fb6d05a864a62a9ad",
            "426842fe13b147e889ebdc06dd826015",
            "6d7c2b3eced046de9354f750f295e95e",
            "091f541c12144eac9bdb601bc99c46c4",
            "2eb2a8645ac34c56b186a7ad0c7d8098",
            "f507044af0f443e5a6a48fc042a02dae",
            "9b416601e47d462bb5f6ecfdd8289663",
            "82af5423db4f4a8f884a29e4f7e1e401",
            "9d70b38e1d024c559d55c6947f9df008",
            "08630178b27240918ee8bd3c7e53fb95",
            "758bef93c1544e6c879797d06c2d0abf",
            "aaa648b2c0504f799727d69e793e6e3f",
            "42e7c9d085a94942a3dd375ae11380f1",
            "7677d39107f5471e8ce6cb0e1acc04a9",
            "68470b04e2a04eb48a1d1bae11e2cd9d",
            "97d1760e2a6d410ea295d6972019bd50",
            "ae975175bdcf4c4d8ecc2b704f714907",
            "388bb19df01c44e884d700c8642821e7",
            "778e81d9be214661b05f3db509a14008",
            "a26de2f7a2d5495ab4f6a968b5993ca6",
            "6891149c05b04c50b3f17f27678ca7d2"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== BASELINE (vLLM only) =====\n",
            "INFO 02-20 16:56:47 [utils.py:261] non-default args: {'enable_prefix_caching': False, 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'model': 'Qwen/Qwen2.5-1.5B-Instruct'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d635326e6f44458e891dbebee355cd26"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 02-20 16:57:26 [model.py:541] Resolved architecture: Qwen2ForCausalLM\n",
            "WARNING 02-20 16:57:26 [model.py:1833] Your device 'Tesla T4' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.\n",
            "WARNING 02-20 16:57:26 [model.py:1885] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 02-20 16:57:26 [model.py:1561] Using max model len 32768\n",
            "INFO 02-20 16:57:27 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "INFO 02-20 16:57:27 [vllm.py:624] Asynchronous scheduling is enabled.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fede554e895743769362628ccb59e31d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "648b44dd505943ecb1945d846b949240"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff314c0dc96d4742a9f244c08975c10d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd927cdc8bfc4c6c92482b1507fe7dc7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "438950fd7cf8411abd543ba98786c0ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 02-20 16:57:44 [system_utils.py:140] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "INFO 02-20 17:03:04 [llm.py:343] Supported tasks: ['generate']\n",
            "vLLM reserved: 11950 MB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Adding requests:   0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b93efeeebeb47fc804bce87af5b79a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processed prompts:   0%|          | 0/20 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "758bef93c1544e6c879797d06c2d0abf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- vLLM Cold Run ---\n",
            "Total time: 163.025 s\n",
            "Latency: 8.15 s\n",
            "Throughput: 0.12 req/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block enables the LMCache connector and internal prefix caching. This allows the system to store the computed KV (Key-Value) states of our massive document in memory rather than re-computing them for every request."
      ],
      "metadata": {
        "id": "tP528B7cdhA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Run vLLM + LMCache\n",
        "# ==========================================\n",
        "print(\"\\n===== vLLM + LMCache =====\")\n",
        "mem_before_lmcache = get_gpu_memory()\n",
        "kv_config = KVTransferConfig(\n",
        "    kv_connector=\"LMCacheConnectorV1\",\n",
        "    kv_role=\"kv_both\"\n",
        ")\n",
        "\n",
        "llm_lmcache = LLM(\n",
        "    model=MODEL_NAME,\n",
        "    gpu_memory_utilization=0.8,\n",
        "    enable_prefix_caching=True,\n",
        "    kv_transfer_config=kv_config\n",
        ")\n",
        "mem_after_lmcache = get_gpu_memory()\n",
        "lmcache_mem_used = mem_after_lmcache - mem_before_lmcache\n",
        "print(f\"vLLM + LMCache reserved: {lmcache_mem_used} MB\")\n",
        "\n",
        "# Cold run to fill cache\n",
        "print(\"\\n>> LMCache Cold Run (Warmup)\")\n",
        "_ = run_benchmark(llm_lmcache, prompts, \"LMCache Cold\")\n",
        "\n",
        "# Warm run (cached)\n",
        "print(\"\\n>> LMCache Warm Run (Cached)\")\n",
        "lmcache_results = run_benchmark(llm_lmcache, prompts, \"LMCache Warm\")\n",
        "\n",
        "# Cleanup\n",
        "del llm_lmcache\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n"
      ],
      "metadata": {
        "id": "Sf93HZd7-9HK",
        "outputId": "8dde455d-6f66-4351-d26d-a59ad56f72c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599,
          "referenced_widgets": [
            "0bee3a0714a9411cb0cd6a99493362d0",
            "2e7f53c3bb5f4a03b91d0233547e7f21",
            "15e1ac77d6eb488587ecf73dc7dbf53b",
            "6e93f891eaa54c58af672c8982da9c15",
            "998f5be0355d4cb48052d6d07923291e",
            "b6f8f241e1044445b7dbe9bb4cf37e95",
            "a7a124f90c1242248c9840cf18a70025",
            "2ca7e9fa4b6d416dab6955c55d7a3943",
            "28a16f8178294ed09f904cf59373baf6",
            "5c075d0cf0854a34b1e6744e1e8477fb",
            "f6025bce23d549289d1b413d6c5068ca",
            "3bbf51c84aa2470ea3c276023c488860",
            "55c2168a3f644d4786777cd907275101",
            "c16277b543f847b1aa228685e3628304",
            "e6d2f7f77d0040fcb4e9b9f2687ec4e3",
            "d0f4ed44416b4de8bcbf90b2ad39066a",
            "0bca7595439d4b4495d14f61255b3062",
            "bde084e7cf5e40d884f1dea181eadbd5",
            "a15c9c4a86a841a29754fb5c5d106a5a",
            "f57fe0a5b8a04a22b8967b8b68a0f20a",
            "073e6d721fc54f9884504ef5d91dc269",
            "c52142beeefd4ea2a83d7dd15f446fd3",
            "45f4adff8b93417a96552d6c090f41ff",
            "d99a33898ea248bea8d434d79ff7d787",
            "e39b068b172548abbb515ae822324ba3",
            "28c42799f5794ab68272e22da7d98ae0",
            "7cf00603b4a24dc3919a52636a23c103",
            "d096c70dba9844919befaefa8f01fccc",
            "5eaba9a4561445a2bda6748c63837477",
            "4ad1ac2e57ee4567997e0b220451fa21",
            "80ae621625734c069d5a9bd4fe887ee8",
            "dde5d2a2332a45a782d1283ed7312e97",
            "4f072f86e2994812a3064c4a50dd52cb",
            "2ed4b10a45b54aadbd9c3537aea341ff",
            "cc2fc17c3d61469c8b16977b15b31ede",
            "676d617b11ea4af0a97e65f529a647f3",
            "c4d86ac331fa4b17bf3cb4f98b30fea6",
            "fd87dd4f8a8648948923442056676585",
            "fa07feb7768a428abd01e5cd4de2b77e",
            "3fbe22a3922b439ca61c19667222d3a8",
            "1adaf7ad2a29424fb741b1905475d18e",
            "acacb78ba34b4e47b4a70e0f2191321a",
            "942286366457479b8d92a78fee65bfe3",
            "191a4fdcd8384737ae7d8408f7439ac8"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== vLLM + LMCache =====\n",
            "INFO 02-20 17:06:07 [utils.py:261] non-default args: {'enable_prefix_caching': True, 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='LMCacheConnectorV1', engine_id='4007737e-49fb-4fc4-8331-8d5a6f742593', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={}, kv_connector_module_path=None, enable_permute_local_kv=False, kv_load_failure_policy='recompute'), 'model': 'Qwen/Qwen2.5-1.5B-Instruct'}\n",
            "INFO 02-20 17:06:08 [model.py:541] Resolved architecture: Qwen2ForCausalLM\n",
            "WARNING 02-20 17:06:08 [model.py:1833] Your device 'Tesla T4' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.\n",
            "WARNING 02-20 17:06:08 [model.py:1885] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 02-20 17:06:08 [model.py:1561] Using max model len 32768\n",
            "INFO 02-20 17:06:08 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 02-20 17:06:08 [vllm.py:980] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py and use --no-disable-hybrid-kv-cache-manager to start vLLM.\n",
            "INFO 02-20 17:07:23 [llm.py:343] Supported tasks: ['generate']\n",
            "vLLM + LMCache reserved: 13416 MB\n",
            "\n",
            ">> LMCache Cold Run (Warmup)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Adding requests:   0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0bee3a0714a9411cb0cd6a99493362d0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processed prompts:   0%|          | 0/20 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3bbf51c84aa2470ea3c276023c488860"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LMCache Cold ---\n",
            "Total time: 14.672 s\n",
            "Latency: 0.73 s\n",
            "Throughput: 1.36 req/s\n",
            "\n",
            ">> LMCache Warm Run (Cached)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Adding requests:   0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45f4adff8b93417a96552d6c090f41ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processed prompts:   0%|          | 0/20 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ed4b10a45b54aadbd9c3537aea341ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LMCache Warm ---\n",
            "Total time: 6.160 s\n",
            "Latency: 0.31 s\n",
            "Throughput: 3.25 req/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It aggregates the performance metrics from both the Baseline (vLLM) and the Optimized (LMCache) runs into a single DataFrame for side-by-side comparison."
      ],
      "metadata": {
        "id": "t49_j505dsqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==========================================\n",
        "# Final comparison with dataframe\n",
        "# ==========================================\n",
        "data = {\n",
        "    \"Method\": [\"vLLM Cold\", \"LMCache Warm\"],\n",
        "    \"Latency (s)\": [baseline_results[\"latency\"], lmcache_results[\"latency\"]],\n",
        "    \"Throughput (req/s)\": [baseline_results[\"throughput\"], lmcache_results[\"throughput\"]],\n",
        "    \"GPU Memory Used (MB)\": [vllm_mem_used, lmcache_mem_used]\n",
        "}\n",
        "\n",
        "df_results = pd.DataFrame(data)\n",
        "print(\"\\n===== FINAL RESULTS =====\")\n",
        "print(df_results)\n",
        "\n",
        "speedup = lmcache_results[\"throughput\"] / baseline_results[\"throughput\"]\n",
        "print(f\"\\nSPEEDUP: {speedup:.2f}x faster with LMCache\")"
      ],
      "metadata": {
        "id": "X2-sw1mA-_7p",
        "outputId": "81900051-1756-4593-f853-99156697e633",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== FINAL RESULTS =====\n",
            "         Method  Latency (s)  Throughput (req/s)  GPU Memory Used (MB)\n",
            "0     vLLM Cold         8.15                0.12                 11950\n",
            "1  LMCache Warm         0.31                3.25                 13416\n",
            "\n",
            "SPEEDUP: 27.08x faster with LMCache\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
 
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
